{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import torch\n",
    "import diart.operators as dops\n",
    "import rx.operators as ops\n",
    "import whisper_timestamped as whisper\n",
    "from diart import OnlineSpeakerDiarization, PipelineConfig\n",
    "from diart.sources import WebSocketAudioSource\n",
    "from pyannote.core import Annotation, SlidingWindowFeature, SlidingWindow, Segment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress whisper-timestamped warnings for a clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "HOST = os.environ.get(\"HOST\")\n",
    "PORT = int(os.environ.get(\"PORT\"))\n",
    "WHISPER_SIZE = os.environ.get(\"WHISPER_SIZE\")\n",
    "WHISPER_COMPRESS_RATIO_THRESHOLD = float(os.environ.get(\"WHISPER_COMPRESS_RATIO_THRESHOLD\"))\n",
    "WHISPER_NO_SPEECH_THRESHOLD = float(os.environ.get(\"WHISPER_NO_SPEECH_THRESHOLD\"))\n",
    "PIPELINE_MAX_SPEAKERS = int(os.environ.get(\"PIPELINE_MAX_SPEAKERS\"))\n",
    "PIPELINE_DURATION = float(os.environ.get(\"PIPELINE_DURATION\"))\n",
    "PIPELINE_STEP = float(os.environ.get(\"PIPELINE_STEP\"))\n",
    "PIPELINE_SAMPLE_RATE = int(os.environ.get(\"PIPELINE_SAMPLE_RATE\"))\n",
    "PIPELINE_TAU = float(os.environ.get(\"PIPELINE_TAU\"))\n",
    "PIPELINE_RHO = float(os.environ.get(\"PIPELINE_RHO\"))\n",
    "PIPELINE_DELTA = float(os.environ.get(\"PIPELINE_DELTA\"))\n",
    "PIPELINE_CHUNK_DURATION = float(os.environ.get(\"PIPELINE_CHUNK_DURATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##helper function to concatenate transcriptions and diarizations\n",
    "def concat(chunks, collar=0.05):\n",
    "    \"\"\"\n",
    "    Concatenate predictions and audio\n",
    "    given a list of `(diarization, waveform)` pairs\n",
    "    and merge contiguous single-speaker regions\n",
    "    with pauses shorter than `collar` seconds.\n",
    "    \"\"\"\n",
    "    first_annotation = chunks[0][0]\n",
    "    first_waveform = chunks[0][1]\n",
    "    annotation = Annotation(uri=first_annotation.uri)\n",
    "    data = []\n",
    "    for ann, wav in chunks:\n",
    "        annotation.update(ann)\n",
    "        data.append(wav.data)\n",
    "    annotation = annotation.support(collar)\n",
    "    window = SlidingWindow(\n",
    "        first_waveform.sliding_window.duration,\n",
    "        first_waveform.sliding_window.step,\n",
    "        first_waveform.sliding_window.start,\n",
    "    )\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    return annotation, SlidingWindowFeature(data, window)\n",
    "\n",
    "##helper function to make different speakers appear as messages\n",
    "def message_transcription(transcription):\n",
    "        \n",
    "    result = []\n",
    "    for speaker, text in transcription:\n",
    "        if speaker == -1:\n",
    "            # No speakerfound for this text, use default terminal color\n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(\"Speaker\"+str(speaker)+\": \"+text)\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    # Auxiliary function to suppress Whisper logs (it is quite verbose)\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model=\"small\", device=None, **kwargs):\n",
    "        self.model = whisper.load_model(model, device=device)\n",
    "        self.kwargs = kwargs\n",
    "        self._buffer = \"\"\n",
    "\n",
    "    def transcribe(self, waveform):\n",
    "        \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "        # Pad/trim audio to fit 30 seconds as required by Whisper\n",
    "        audio = waveform.data.astype(\"float32\").reshape(-1)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "        # Transcribe the given audio while suppressing logs\n",
    "        with suppress_stdout():\n",
    "            transcription = whisper.transcribe(\n",
    "                self.model,\n",
    "                audio,\n",
    "                # We use past transcriptions to condition the model\n",
    "                initial_prompt=self._buffer,\n",
    "                verbose=True,  # to avoid progress bar\n",
    "                **self.kwargs\n",
    "            )\n",
    "\n",
    "        return transcription\n",
    "    \n",
    "    def identify_speakers(self, transcription, diarization, time_shift):\n",
    "        \"\"\"Iterate over transcription segments to assign speakers\"\"\"\n",
    "        speaker_captions = []\n",
    "        for segment in transcription[\"segments\"]:\n",
    "\n",
    "            # Crop diarization to the segment timestamps\n",
    "            start = time_shift + segment[\"words\"][0][\"start\"]\n",
    "            end = time_shift + segment[\"words\"][-1][\"end\"]\n",
    "            dia = diarization.crop(Segment(start, end))\n",
    "\n",
    "            # Assign a speaker to the segment based on diarization\n",
    "            speakers = dia.labels()\n",
    "            num_speakers = len(speakers)\n",
    "            if num_speakers == 0:\n",
    "                # No speakers were detected\n",
    "                caption = (-1, segment[\"text\"])\n",
    "            elif num_speakers == 1:\n",
    "                # Only one speaker is active in this segment\n",
    "                spk_id = int(speakers[0].split(\"speaker\")[1])\n",
    "                caption = (spk_id, segment[\"text\"])\n",
    "            else:\n",
    "                # Multiple speakers, select the one that speaks the most\n",
    "                max_speaker = int(np.argmax([\n",
    "                    dia.label_duration(spk) for spk in speakers\n",
    "                ]))\n",
    "                caption = (max_speaker, segment[\"text\"])\n",
    "            speaker_captions.append(caption)\n",
    "\n",
    "        return speaker_captions\n",
    "\n",
    "    def __call__(self, diarization, waveform):\n",
    "        # Step 1: Transcribe\n",
    "        transcription = self.transcribe(waveform)\n",
    "        # Update transcription buffer\n",
    "        self._buffer += transcription[\"text\"]\n",
    "        # The audio may not be the beginning of the conversation\n",
    "        time_shift = waveform.sliding_window.start\n",
    "        # Step 2: Assign speakers\n",
    "        speaker_transcriptions = self.identify_speakers(transcription, diarization, time_shift)\n",
    "        return speaker_transcriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline params, haven't tinkered with them much, you can also set device=torch.device(\"cuda\")\n",
    "config = PipelineConfig(\n",
    "    max_speakers=PIPELINE_MAX_SPEAKERS,\n",
    "    duration=PIPELINE_DURATION,\n",
    "    step=PIPELINE_STEP,\n",
    "    sample_rate=PIPELINE_SAMPLE_RATE,\n",
    "    latency=\"min\",\n",
    "    tau_active=PIPELINE_TAU,\n",
    "    rho_update=PIPELINE_RHO,\n",
    "    delta_new=PIPELINE_DELTA,\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# Set the diarization model\n",
    "dia = OnlineSpeakerDiarization(config)\n",
    "# Set the remote audio source\n",
    "source = WebSocketAudioSource(config.sample_rate, HOST, PORT)\n",
    "# Set the whisper model size, you can also set device=\"cuda\"\n",
    "asr = WhisperTranscriber(model=WHISPER_SIZE, \n",
    "                         device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         compression_ratio_threshold=WHISPER_COMPRESS_RATIO_THRESHOLD,\n",
    "                         no_speech_threshold=WHISPER_NO_SPEECH_THRESHOLD)\n",
    "\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(PIPELINE_CHUNK_DURATION // config.step)\n",
    "\n",
    "# Chain of operations to test message helper for the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(config.duration, config.step, config.sample_rate),\n",
    "    # Wait until a batch is full - The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction - The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(dia),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(concat),\n",
    "    # Ignore this chunk if it does not contain speech\n",
    "    ops.filter(lambda ann_wav: ann_wav[0].get_timeline().duration() > 0),\n",
    "    # Obtain speaker-aware transcriptions - The output is a list of pairs `(speaker: int, caption: str)`\n",
    "    ops.starmap(asr),\n",
    "    # Color transcriptions according to the speaker - The output is plain text with color references for rich\n",
    "    ops.map(message_transcription),\n",
    ").subscribe(\n",
    "    on_next=print,  # print colored text\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the magic happen\n",
    "print(\"Listening to remote microphone...\")\n",
    "source.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
