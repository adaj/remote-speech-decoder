{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd11db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4c4a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##spicier imports\n",
    "import diart.operators as dops\n",
    "import numpy as np\n",
    "import rich\n",
    "import rx.operators as ops\n",
    "import whisper_timestamped as whisper\n",
    "from diart import OnlineSpeakerDiarization, PipelineConfig\n",
    "from diart.sources import MicrophoneAudioSource\n",
    "from pyannote.core import Annotation, SlidingWindowFeature, SlidingWindow, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##helper function to concatenate transcriptions and diarizations\n",
    "def concat(chunks, collar=0.05):\n",
    "    \"\"\"\n",
    "    Concatenate predictions and audio\n",
    "    given a list of `(diarization, waveform)` pairs\n",
    "    and merge contiguous single-speaker regions\n",
    "    with pauses shorter than `collar` seconds.\n",
    "    \"\"\"\n",
    "    first_annotation = chunks[0][0]\n",
    "    first_waveform = chunks[0][1]\n",
    "    annotation = Annotation(uri=first_annotation.uri)\n",
    "    data = []\n",
    "    for ann, wav in chunks:\n",
    "        annotation.update(ann)\n",
    "        data.append(wav.data)\n",
    "    annotation = annotation.support(collar)\n",
    "    window = SlidingWindow(\n",
    "        first_waveform.sliding_window.duration,\n",
    "        first_waveform.sliding_window.step,\n",
    "        first_waveform.sliding_window.start,\n",
    "    )\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    return annotation, SlidingWindowFeature(data, window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8790dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##helper function to make different speakers appear as messages\n",
    "def message_transcription(transcription):\n",
    "        \n",
    "    result = []\n",
    "    for speaker, text in transcription:\n",
    "        if speaker == -1:\n",
    "            # No speakerfound for this text, use default terminal color\n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(\"Speaker\"+str(speaker)+\": \"+text)\n",
    "    return \"\\n\".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2572407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    # Auxiliary function to suppress Whisper logs (it is quite verbose)\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model=\"small\", device=None):\n",
    "        self.model = whisper.load_model(model, device=device)\n",
    "        self._buffer = \"\"\n",
    "\n",
    "    def transcribe(self, waveform):\n",
    "        \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "        # Pad/trim audio to fit 30 seconds as required by Whisper\n",
    "        audio = waveform.data.astype(\"float32\").reshape(-1)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "        # Transcribe the given audio while suppressing logs\n",
    "        with suppress_stdout():\n",
    "            transcription = whisper.transcribe(\n",
    "                self.model,\n",
    "                audio,\n",
    "                # We use past transcriptions to condition the model\n",
    "                initial_prompt=self._buffer,\n",
    "                verbose=True,  # to avoid progress bar\n",
    "                ##decode_options=options\n",
    "            )\n",
    "\n",
    "        return transcription\n",
    "    \n",
    "    def identify_speakers(self, transcription, diarization, time_shift):\n",
    "        \"\"\"Iterate over transcription segments to assign speakers\"\"\"\n",
    "        speaker_captions = []\n",
    "        for segment in transcription[\"segments\"]:\n",
    "\n",
    "            # Crop diarization to the segment timestamps\n",
    "            start = time_shift + segment[\"words\"][0][\"start\"]\n",
    "            end = time_shift + segment[\"words\"][-1][\"end\"]\n",
    "            dia = diarization.crop(Segment(start, end))\n",
    "\n",
    "            # Assign a speaker to the segment based on diarization\n",
    "            speakers = dia.labels()\n",
    "            num_speakers = len(speakers)\n",
    "            if num_speakers == 0:\n",
    "                # No speakers were detected\n",
    "                caption = (-1, segment[\"text\"])\n",
    "            elif num_speakers == 1:\n",
    "                # Only one speaker is active in this segment\n",
    "                spk_id = int(speakers[0].split(\"speaker\")[1])\n",
    "                caption = (spk_id, segment[\"text\"])\n",
    "            else:\n",
    "                # Multiple speakers, select the one that speaks the most\n",
    "                max_speaker = int(np.argmax([\n",
    "                    dia.label_duration(spk) for spk in speakers\n",
    "                ]))\n",
    "                caption = (max_speaker, segment[\"text\"])\n",
    "            speaker_captions.append(caption)\n",
    "\n",
    "        return speaker_captions\n",
    "\n",
    "    def __call__(self, diarization, waveform):\n",
    "        # Step 1: Transcribe\n",
    "        transcription = self.transcribe(waveform)\n",
    "        # Update transcription buffer\n",
    "        self._buffer += transcription[\"text\"]\n",
    "        # The audio may not be the beginning of the conversation\n",
    "        time_shift = waveform.sliding_window.start\n",
    "        # Step 2: Assign speakers\n",
    "        speaker_transcriptions = self.identify_speakers(transcription, diarization, time_shift)\n",
    "        return speaker_transcriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa86ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress whisper-timestamped warnings for a clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94bc2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline params. haven't tinkered with them much. you can also set device=torch.device(\"cuda\")\n",
    "config = PipelineConfig(\n",
    "    duration=5,\n",
    "    step=0.5,\n",
    "    latency=\"min\",\n",
    "    tau_active=0.5,\n",
    "    rho_update=0.1,\n",
    "    delta_new=0.57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9c09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up sources for both modules\n",
    "dia = OnlineSpeakerDiarization(config)\n",
    "source = MicrophoneAudioSource(config.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b8cc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the whisper model size, you can also set device=\"cuda\"\n",
    "asr = WhisperTranscriber(model=\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cefa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the stream into 2s chunks for transcription\n",
    "transcription_duration = 2\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(transcription_duration // config.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce27aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rx.disposable.disposable.Disposable at 0x17e0f978610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain of operations to test message helper for the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(\n",
    "        config.duration, config.step, config.sample_rate\n",
    "    ),\n",
    "    # Wait until a batch is full\n",
    "    # The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction\n",
    "    # The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(dia),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(concat),\n",
    "    # Ignore this chunk if it does not contain speech\n",
    "    ops.filter(lambda ann_wav: ann_wav[0].get_timeline().duration() > 0),\n",
    "    # Obtain speaker-aware transcriptions\n",
    "    # The output is a list of pairs `(speaker: int, caption: str)`\n",
    "    ops.starmap(asr),\n",
    "    # Color transcriptions according to the speaker\n",
    "    # The output is plain text with color references for rich\n",
    "    ops.map(message_transcription),\n",
    ").subscribe(\n",
    "    on_next=rich.print,  # print colored text\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebeb3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> for\n",
       "</pre>\n"
      ],
      "text/plain": [
       " for\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Speaker0:  starts printing out the words that I'm saying.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Speaker0:  starts printing out the words that I'm saying.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Speaker0:  So I'm just talking.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Speaker0:  So I'm just talking.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Speaker0:  off the top of my head to keep things\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Speaker0:  off the top of my head to keep things\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> going. Normally when it's\n",
       "</pre>\n"
      ],
      "text/plain": [
       " going. Normally when it's\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> giving single words like that, it, oh, it's\n",
       "</pre>\n"
      ],
      "text/plain": [
       " giving single words like that, it, oh, it's\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\diart\\sources.py\", line 173, in read\n",
      "    self.stream.on_next(self._queue.get_nowait())\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\subject\\subject.py\", line 55, in on_next\n",
      "    super().on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\observer.py\", line 26, in on_next\n",
      "    self._on_next_core(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\subject\\subject.py\", line 62, in _on_next_core\n",
      "    observer.on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 41, in on_next\n",
      "    obv.on_next(result)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\filter.py\", line 34, in on_next\n",
      "    observer.on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\filter.py\", line 34, in on_next\n",
      "    observer.on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 41, in on_next\n",
      "    obv.on_next(result)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\windowwithcount.py\", line 61, in on_next\n",
      "    s.on_completed()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\subject\\subject.py\", line 89, in on_completed\n",
      "    super().on_completed()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\observer.py\", line 56, in on_completed\n",
      "    self._on_completed_core()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\subject\\subject.py\", line 97, in _on_completed_core\n",
      "    observer.on_completed()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 44, in on_completed\n",
      "    self._on_completed()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 44, in on_completed\n",
      "    self._on_completed()\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\toiterable.py\", line 24, in on_completed\n",
      "    observer.on_next(queue)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 41, in on_next\n",
      "    obv.on_next(result)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\internal\\concurrency.py\", line 16, in inner\n",
      "    return fn(*args, **kw)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\filter.py\", line 34, in on_next\n",
      "    observer.on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 41, in on_next\n",
      "    obv.on_next(result)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 41, in on_next\n",
      "    obv.on_next(result)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\filter.py\", line 34, in on_next\n",
      "    observer.on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\observer\\autodetachobserver.py\", line 26, in on_next\n",
      "    self._on_next(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\core\\operators\\map.py\", line 37, in on_next\n",
      "    result = _mapper(value)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\rx\\operators\\__init__.py\", line 2662, in <lambda>\n",
      "    return pipe(map(lambda values: cast(Mapper, mapper)(*values)))\n",
      "  File \"C:\\Users\\jedwards23\\AppData\\Local\\Temp\\ipykernel_25084\\679246776.py\", line 69, in __call__\n",
      "    transcription = self.transcribe(waveform)\n",
      "  File \"C:\\Users\\jedwards23\\AppData\\Local\\Temp\\ipykernel_25084\\679246776.py\", line 26, in transcribe\n",
      "    transcription = whisper.transcribe(\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper_timestamped\\transcribe.py\", line 266, in transcribe_timestamped\n",
      "    (transcription, words) = _transcribe_timestamped_efficient(model, audio,\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper_timestamped\\transcribe.py\", line 846, in _transcribe_timestamped_efficient\n",
      "    transcription = model.transcribe(audio, **whisper_options)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\transcribe.py\", line 229, in transcribe\n",
      "    result: DecodingResult = decode_with_fallback(mel_segment)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\transcribe.py\", line 164, in decode_with_fallback\n",
      "    decode_result = model.decode(segment, options)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\decoding.py\", line 811, in decode\n",
      "    result = DecodingTask(model, options).run(mel)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\decoding.py\", line 704, in run\n",
      "    audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\decoding.py\", line 640, in _get_audio_features\n",
      "    audio_features = self.model.encoder(mel)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\model.py\", line 170, in forward\n",
      "    x = block(x)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\model.py\", line 136, in forward\n",
      "    x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\model.py\", line 90, in forward\n",
      "    wv, qk = self.qkv_attention(q, k, v, mask)\n",
      "  File \"C:\\Users\\jedwards23\\Anaconda3\\envs\\diart\\lib\\site-packages\\whisper\\model.py\", line 108, in qkv_attention\n",
      "    return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "## Make the magic happen\n",
    "print(\"Listening...\")\n",
    "source.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
